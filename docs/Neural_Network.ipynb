{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448928b2",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# ***Neural network***\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9429803",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7853672",
   "metadata": {},
   "source": [
    "\n",
    "<figure align=\"center\">\n",
    "    <img src=\"../assets/images/neural_network_implement_tensorflow.png\">\n",
    "    <p> Fig 1. Implement tensorflow </p>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811ccdd",
   "metadata": {},
   "source": [
    "- Ở hình trên ta có thể thấy rằng: Một neural network(NN) đơn giản có 4 layer gồm có \n",
    "    + Layer 0                     - input layer\n",
    "    + Layer 1: 25 units (neurons) - hidden layer\n",
    "    + Layer 2: 15 units (neurons) - hidden layer\n",
    "    + Layer 3: 1 units  (neuron)  - output layer\n",
    "\n",
    "- Code trên: implement this basic neuron network\n",
    "    + Sequential: các layer xếp theo thứ tự từ trái sang layer1 -> layer 2 -> ...\n",
    "        Điều này có nghĩa rằng ouput của layer này sẽ là input của layer kế tiếp nó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e5f35",
   "metadata": {},
   "source": [
    "## 1. Batch size & Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ce3ce",
   "metadata": {},
   "source": [
    "- Trong slide trên thầy A.Ng có note rằng: \"epochs: number of steps in gradient descent\"\n",
    "    + Có thể hiểu rằng ở đây ta đang dùng `full batch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599857a",
   "metadata": {},
   "source": [
    "### 1.2 Epoch: \n",
    "epoch &harr; toàn bộ dataset, với epoch = 10, nghĩa là model học qua toàn bộ dataset 10 lần."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fa5de",
   "metadata": {},
   "source": [
    "### 1.1 Batch: \n",
    "chia dataset thành các sub_dataset\n",
    "- Ví dụ: ta có dataset với số lượng samples = 1.000.000, ta chia 100 batch\n",
    "Khi đó mỗi batch có 10.000 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd616f",
   "metadata": {},
   "source": [
    "#### `Tại sao lại phải chia batch-size như thế?`\n",
    "\n",
    "Như ví dụ trên, dataset có 1.000.000 samples, một số lượng khá lớn. \n",
    "- Nếu như không chia *batch*, khi train:\n",
    "    + Mỗi lần model sẽ phải tính toán trên toàn dataset &rarr; khối lượng tính toán và dung lượng bộ nhớ cần thiết là rất nhiều\n",
    "    + Không tận dụng được sức mạnh tính toán song song (parallel), liên tục \n",
    "    + Mất thời gian rất lâu để có thể cập nhật w, b (gradient descent)\n",
    "    + Có thể mắc kẹt ở `local minimum`  \n",
    "\n",
    "- Nếu chia *batch* thì có thể khắc phục được các nhược điểm kể trên đối với full-batch:\n",
    "    + Cập nhật *gradient* liên tục\n",
    "    + Khối lượng tính toán vừa phải và song song, liên tục\n",
    "    + Có thể thoát khỏi `local minimum` để tới `global minimum`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0102c3",
   "metadata": {},
   "source": [
    "##### `local minimum` & `global minimum`\n",
    "\n",
    "Mở đầu đơn giản với Linear Regression *f(x) = wx + b*, ta mong muốn hàm số khớp nhất với data, đo bằng hàm mất mát, *J(w, b)*. Biểu đồ thể hiện sự tương quan giữa w và J có thể hình dung đơn giản như một parabol - cái bát ngửa, ta cần tìm *điểm cực tiểu* của đô thị này, có thể sử dụng tóan học, đạo hàm để tính ra; song, tổng quát hơn khi một hàm *J(w, b)* phức tạp thì điều này không khả thi, thay vào đó ta sử dụng phương pháp *Gradient Descent* cập nhật liên tục w, b. \n",
    "\n",
    "<figure align=\"center\">\n",
    "    <img src=\"../assets/images/local_vs_global_minima.png\" width=\"1000px\" height=\"700px\"> <br>\n",
    "    <a href=\"https://blog.goodaudience.com/gradient-descent-for-linear-regression-explained-7c60bc414bdd\"> Fig 2. Local vs Global Minima </a>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Nâng cao hơn, khi đồ thị của hàm *J* phức tạp hơn, như một mặt phẳng có nhiều chỗ lõm khác nhau, chỗ thì lõm ít, chỗ thì lõm nhiều, hay đó chính là local và global minimum (chỗ lõm nhiều nhất). Nếu ta sử dụng *full-batch*, nếu điểm bắt đầu không tốt, rất có thể nó sẽ hội tụ tại `local minimum` và không có *lực đẩy* ra vì ta đang xét trên toàn bộ dataset. Còn nếu sử dụng *batch* thì mỗi lần cập nhật như thế, chỉ là một phần của dataset, khi batch này đưa ta xuống local thì batch khác cho gradient khác, hướng khác để ta thoát ra khỏi local này. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85c9e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e76e71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed6bcb3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
