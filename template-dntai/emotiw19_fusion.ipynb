{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOTIW19 - FUSION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2, glob, argparse\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython import display\n",
    "\n",
    "import os, sys, re\n",
    "root_dir = '/mnt/ubuntu1/dntai/projects_merge/EmotiWPrj/'\n",
    "lib_dirs = [os.path.join(root_dir, 'sources')]\n",
    "for lib in lib_dirs:\n",
    "    if lib not in sys.path: sys.path.insert(0, lib)\n",
    "# np.set_printoptions(precision=2, suppress=True, formatter={'float': '{: 0.4f}'.format}, linewidth=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from emotiw19prj.fusion.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "+ frame_root_path: /mnt/ubuntu1/dntai/projects/datasets/emotiw/AFEW19/frames\n",
      "+ db_file: /mnt/ubuntu1/dntai/projects/datasets/emotiw/AFEW19/emotiw19_trainvalid_person_png.h5\n",
      "+ batch_size: 8\n",
      "+ result_dir: /mnt/ubuntu1/dntai/projects_merge/EmotiWPrj/data/results/emotiw19_fusion\n",
      "+ gpus: ['3']\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "parser = argparse.ArgumentParser(description='Facial Emotion Recognition - EmotiW2019')\n",
    "parser.add_argument('--gpus',   type=str, default=\"['3']\", help='')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='')\n",
    "\n",
    "parser.add_argument('--frame_root_path' , type=str  , default=\"/mnt/ubuntu1/dntai/projects/datasets/emotiw/AFEW19/frames\", help='')\n",
    "parser.add_argument('--db_file' , type=str  , default=\"/mnt/ubuntu1/dntai/projects/datasets/emotiw/AFEW19/emotiw19_trainvalid_person_png.h5\", help='')\n",
    "\n",
    "parser.add_argument('--result_dir', default=root_dir + \"data/results/emotiw19_fusion\", \n",
    "                    type=str, metavar='result_dir', help='')\n",
    "\n",
    "# # Audio Features \n",
    "parser.add_argument('--audio_features_dir', default=root_dir + \"data/results/feature3d/densenet_scale124_mean_190621_01\", \n",
    "                    type=str, metavar='runtime_dir', help='')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "params  = vars(args)\n",
    "params[\"gpus\"] = eval(params[\"gpus\"])\n",
    "print(\"Parameters: \")\n",
    "for key in params.keys(): print(\"+ %s: %s\"%(key,params[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment GPUs:\n",
      "+ Choose GPUs:  3\n",
      "+ Keras backend:  tensorflow\n",
      "TensorFlow Version: 1.9.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Choose gpus, init session\n",
    "choose_keras_environment(gpus = params[\"gpus\"], keras_backend = \"tensorflow\", verbose = 1)\n",
    "init_session()\n",
    "check_tensorflow_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/emotion', '/faces', '/frames', '/person', '/video']\n",
      "Number of training:  773\n",
      "Number of validating:  383\n"
     ]
    }
   ],
   "source": [
    "emotiw_name = np.array([\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"])\n",
    "emotiw_sdict = dict(zip(emotiw_name, np.arange(len(emotiw_name))))\n",
    "\n",
    "df_all = pd.HDFStore(params[\"db_file\"], \"r\")\n",
    "print(df_all.keys())\n",
    "df_emotion = df_all[\"emotion\"]\n",
    "df_video   = df_all[\"video\"]\n",
    "df_frames  = df_all[\"frames\"]\n",
    "df_faces   = df_all[\"faces\"]\n",
    "df_person  = df_all[\"person\"]\n",
    "df_all.close()\n",
    "\n",
    "df_train = df_video.query(\"type=='train'\")\n",
    "df_valid = df_video.query(\"type=='valid'\")\n",
    "\n",
    "print(\"Number of training: \", len(df_train))\n",
    "print(\"Number of validating: \", len(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  {'Happy': 63, 'Sad': 61, 'Neutral': 63, 'Surprise': 46, 'Angry': 64, 'Disgust': 40, 'Fear': 46}\n",
      "Training:  {'Happy': 150, 'Sad': 117, 'Neutral': 144, 'Surprise': 74, 'Angry': 133, 'Disgust': 74, 'Fear': 81}\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating: \", dict(df_valid.groupby([\"emotion\"])[\"name\"].count().reset_index().values))\n",
    "print(\"Training: \", dict(df_train.groupby([\"emotion\"])[\"name\"].count().reset_index().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  ['000029301' '000046280' '000059880' '000102534']  ...\n",
      "Valid:  ['000123880' '000142325' '000147200' '000149120']  ...\n"
     ]
    }
   ],
   "source": [
    "x_train_data = df_train[\"name\"].values\n",
    "y_train_data = np.array([emotiw_sdict[v] for v in df_train[\"emotion\"].values])\n",
    "x_valid_data = df_valid[\"name\"].values\n",
    "y_valid_data = np.array([emotiw_sdict[v] for v in df_valid[\"emotion\"].values])\n",
    "\n",
    "idx_train_video_sorted = np.argsort(x_train_data)\n",
    "train_video_names      = x_train_data[idx_train_video_sorted]\n",
    "train_emotiw_data      = y_train_data[idx_train_video_sorted]\n",
    "\n",
    "idx_valid_video_sorted = np.argsort(x_valid_data)\n",
    "valid_video_names      = x_valid_data[idx_valid_video_sorted]\n",
    "valid_emotiw_data      = y_valid_data[idx_valid_video_sorted]\n",
    "\n",
    "train_name_s2i = dict(zip(train_video_names, range(len(train_video_names))))\n",
    "valid_name_s2i = dict(zip(valid_video_names, range(len(valid_video_names))))\n",
    "\n",
    "print(\"Train: \", train_video_names[0:4], \" ...\")\n",
    "print(\"Valid: \", valid_video_names[0:4], \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cur35",
   "language": "python",
   "name": "cur35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
